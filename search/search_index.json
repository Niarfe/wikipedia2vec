{
    "docs": [
        {
            "location": "/",
            "text": "Wikipedia2Vec\n\n\n\n\n\nStar\n\n\nIntroduction\n\uf0c1\n\n\nWikipedia2Vec is a tool used for obtaining embeddings (vector representations) of words and entities from Wikipedia.\nIt is developed and maintained by \nStudio Ousia\n.\n\n\nThis tool enables you to learn embeddings that map words and entities into a unified continuous vector space.\nThe embeddings can be used as word embeddings, entity embeddings, and the unified embeddings of words and entities.\nThey are used in the state-of-the-art models of various tasks such as \nentity linking\n, \nnamed entity recognition\n, \nentity relatedness\n, and \nquestion answering\n.\n\n\nThe embeddings can be easily trained by a single command with a publicly available Wikipedia dump as input.\nThe code is implemented in Python, and optimized using Cython and BLAS.\n\n\nPretrained embeddings for 12 languages can be downloaded from \nthis page\n.\n\n\nReference\n\uf0c1\n\n\nIf you use Wikipedia2Vec in a scientific publication, please cite the following paper:\n\n\n@InProceedings{yamada-EtAl:2016:CoNLL,\n  author    = {Yamada, Ikuya  and  Shindo, Hiroyuki  and  Takeda, Hideaki  and  Takefuji, Yoshiyasu},\n  title     = {Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation},\n  booktitle = {Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning},\n  month     = {August},\n  year      = {2016},\n  address   = {Berlin, Germany},\n  pages     = {250--259},\n  publisher = {Association for Computational Linguistics}\n}\n\n\n\n\nLicense\n\uf0c1\n\n\nApache License 2.0",
            "title": "Home"
        },
        {
            "location": "/#introduction",
            "text": "Wikipedia2Vec is a tool used for obtaining embeddings (vector representations) of words and entities from Wikipedia.\nIt is developed and maintained by  Studio Ousia .  This tool enables you to learn embeddings that map words and entities into a unified continuous vector space.\nThe embeddings can be used as word embeddings, entity embeddings, and the unified embeddings of words and entities.\nThey are used in the state-of-the-art models of various tasks such as  entity linking ,  named entity recognition ,  entity relatedness , and  question answering .  The embeddings can be easily trained by a single command with a publicly available Wikipedia dump as input.\nThe code is implemented in Python, and optimized using Cython and BLAS.  Pretrained embeddings for 12 languages can be downloaded from  this page .",
            "title": "Introduction"
        },
        {
            "location": "/#reference",
            "text": "If you use Wikipedia2Vec in a scientific publication, please cite the following paper:  @InProceedings{yamada-EtAl:2016:CoNLL,\n  author    = {Yamada, Ikuya  and  Shindo, Hiroyuki  and  Takeda, Hideaki  and  Takefuji, Yoshiyasu},\n  title     = {Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation},\n  booktitle = {Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning},\n  month     = {August},\n  year      = {2016},\n  address   = {Berlin, Germany},\n  pages     = {250--259},\n  publisher = {Association for Computational Linguistics}\n}",
            "title": "Reference"
        },
        {
            "location": "/#license",
            "text": "Apache License 2.0",
            "title": "License"
        },
        {
            "location": "/features/",
            "text": "Features\n\uf0c1\n\n\n\n\nWikipedia2Vec has two unique features that are designed to effectively learn embeddings from Wikipedia.\n\n\nExtended Skip-Gram Model to Learn Embeddings of Words and Entities\n\uf0c1\n\n\n\n\nWikipedia2Vec is based on the \nWord2vec's skip-gram model\n that learns to predict neighboring words given each word in corpora.\nWe extend the skip-gram model by adding the following two submodels:\n\n\n\n\nThe link graph model\n that learns to estimate neighboring entities given an entity in the link graph of Wikipedia entities.\n\n\nThe anchor context model\n that learns to predict neighboring words given an entity by using a link that points to the entity and its neighboring words.\n\n\n\n\nBy jointly optimizing the skip-gram model and these two submodels, our model simultaneously learns the embedding of words and entities from Wikipedia.\nFor further details, please refer to our paper: \nJoint Learning of the Embedding of Words and Entities for Named Entity Disambiguation\n.\n\n\nAutomatic Generation of Entity Links\n\uf0c1\n\n\nMany entity names in Wikipedia do not appear as links because Wikipedia instructs its contributors \nto link an entity name if it is the first occurrence in the page\n.\nThis is problematic for our model because the anchor context model depends on entity links to generate contextual words of entities.\n\n\nTo address this, we implement a feature that automatically links entity names that do not appear as links.\nIn particular, it takes all words and phrases, and treats them as candidates of entity names if they exist in the \nMention DB\n that contains mapping of an entity name (e.g., \u201cWashington\u201d) to a set of possible referent entities (e.g., Washington, D.C. and George Washington).\nThen, it converts an entity name to a link pointing to an entity if the entity name is unambiguous (i.e., there is only one referent entity associated to the entity name in the DB) or the entity is referred by an entity link in the same page.",
            "title": "Features"
        },
        {
            "location": "/features/#features",
            "text": "Wikipedia2Vec has two unique features that are designed to effectively learn embeddings from Wikipedia.",
            "title": "Features"
        },
        {
            "location": "/features/#extended-skip-gram-model-to-learn-embeddings-of-words-and-entities",
            "text": "Wikipedia2Vec is based on the  Word2vec's skip-gram model  that learns to predict neighboring words given each word in corpora.\nWe extend the skip-gram model by adding the following two submodels:   The link graph model  that learns to estimate neighboring entities given an entity in the link graph of Wikipedia entities.  The anchor context model  that learns to predict neighboring words given an entity by using a link that points to the entity and its neighboring words.   By jointly optimizing the skip-gram model and these two submodels, our model simultaneously learns the embedding of words and entities from Wikipedia.\nFor further details, please refer to our paper:  Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation .",
            "title": "Extended Skip-Gram Model to Learn Embeddings of Words and Entities"
        },
        {
            "location": "/features/#automatic-generation-of-entity-links",
            "text": "Many entity names in Wikipedia do not appear as links because Wikipedia instructs its contributors  to link an entity name if it is the first occurrence in the page .\nThis is problematic for our model because the anchor context model depends on entity links to generate contextual words of entities.  To address this, we implement a feature that automatically links entity names that do not appear as links.\nIn particular, it takes all words and phrases, and treats them as candidates of entity names if they exist in the  Mention DB  that contains mapping of an entity name (e.g., \u201cWashington\u201d) to a set of possible referent entities (e.g., Washington, D.C. and George Washington).\nThen, it converts an entity name to a link pointing to an entity if the entity name is unambiguous (i.e., there is only one referent entity associated to the entity name in the DB) or the entity is referred by an entity link in the same page.",
            "title": "Automatic Generation of Entity Links"
        },
        {
            "location": "/install/",
            "text": "Installation\n\uf0c1\n\n\n\n\nWikipedia2Vec can be installed from PyPI:\n\n\n% pip install wikipedia2vec\n\n\n\n\nAlternatively, you can install the development version of this software from the GitHub repository:\n\n\n% git clone https://github.com/studio-ousia/wikipedia2vec.git\n% cd wikipedia2vec\n% pip install Cython\n% ./cythonize.sh\n% pip install .\n\n\n\n\nWikipedia2Vec requires the 64-bit version of Python, and can be run on Linux, Windows, and Mac OSX.\nIt currently depends on the following Python libraries: \nclick\n, \njieba\n, \njoblib\n, \nlmdb\n, \nmarisa-trie\n, \nmwparserfromhell\n, \nnumpy\n, \nscipy\n, \nsix\n, and \ntqdm\n.\n\n\nIf you want to train embeddings on your machine, it is highly recommended to install a BLAS library.\nWe recommend using \nOpenBLAS\n or \nIntel Math Kernel Library\n.\nNote that, the BLAS library needs to be recognized properly from SciPy.\nThis can be confirmed by using the following command:\n\n\n% python -c 'import scipy; scipy.show_config()'\n\n\n\n\nTo process Japanese Wikipedia dumps, it is also required to install \nMeCab\n and \nits Python binding\n.\nFurthermore, to use \nICU library\n to split either words or sentences or both, you need to install the \nC/C++ ICU library\n and the \nPyICU\n library.",
            "title": "Installation"
        },
        {
            "location": "/install/#installation",
            "text": "Wikipedia2Vec can be installed from PyPI:  % pip install wikipedia2vec  Alternatively, you can install the development version of this software from the GitHub repository:  % git clone https://github.com/studio-ousia/wikipedia2vec.git\n% cd wikipedia2vec\n% pip install Cython\n% ./cythonize.sh\n% pip install .  Wikipedia2Vec requires the 64-bit version of Python, and can be run on Linux, Windows, and Mac OSX.\nIt currently depends on the following Python libraries:  click ,  jieba ,  joblib ,  lmdb ,  marisa-trie ,  mwparserfromhell ,  numpy ,  scipy ,  six , and  tqdm .  If you want to train embeddings on your machine, it is highly recommended to install a BLAS library.\nWe recommend using  OpenBLAS  or  Intel Math Kernel Library .\nNote that, the BLAS library needs to be recognized properly from SciPy.\nThis can be confirmed by using the following command:  % python -c 'import scipy; scipy.show_config()'  To process Japanese Wikipedia dumps, it is also required to install  MeCab  and  its Python binding .\nFurthermore, to use  ICU library  to split either words or sentences or both, you need to install the  C/C++ ICU library  and the  PyICU  library.",
            "title": "Installation"
        },
        {
            "location": "/commands/",
            "text": "Learning Embeddings\n\uf0c1\n\n\n\n\nFirst, you need to download a source Wikipedia dump file (e.g., enwiki-latest-pages-articles.xml.bz2) from \nWikimedia Downloads\n.\nThe English dump file can be obtained by running the following command.\n\n\n% wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2\n\n\n\n\nNote that you do not need to decompress the dump file.\n\n\nThen, the embeddings can be trained from a Wikipedia dump using the \ntrain\n command.\n\n\n% wikipedia2vec train DUMP_FILE OUT_FILE\n\n\n\n\nArguments:\n\n\n\n\nDUMP_FILE\n: The Wikipedia dump file\n\n\nOUT_FILE\n: The output file\n\n\n\n\nOptions:\n\n\n\n\n--dim-size\n: The number of dimensions of the embeddings (default: 100)\n\n\n--window\n: The maximum distance between the target item (word or entity) and the context word to be predicted (default: 5)\n\n\n--iteration\n: The number of iterations for Wikipedia pages (default: 5)\n\n\n--negative\n: The number of negative samples (default: 5)\n\n\n--lowercase/--no-lowercase\n: Whether to lowercase words (default: True)\n\n\n--tokenizer\n: The name of the tokenizer used to tokenize a text into words. Possible choices are \nregexp\n, \nicu\n, \nmecab\n, and \njieba\n\n\n--sent-detect\n: The sentence detector used to split texts into sentences. Currently, only \nicu\n is the possible value (default: None)\n\n\n--min-word-count\n: A word is ignored if the total frequency of the word is less than this value (default: 10)\n\n\n--min-entity-count\n: An entity is ignored if the total frequency of the entity appearing as the referent of an anchor link is less than this value (default: 5)\n\n\n--min-paragraph-len\n: A paragraph is ignored if its length is shorter than this value (default: 5)\n\n\n--category/--no-category\n: Whether to include Wikipedia categories in the dictionary (default:False)\n\n\n--disambi/--no-disambi\n: Whether to include disambiguation entities in the dictionary (default:False)\n\n\n--link-graph/--no-link-graph\n: Whether to learn from the Wikipedia link graph (default: True)\n\n\n--entities-per-page\n: For processing each page, the specified number of randomly chosen entities are used to predict their neighboring entities in the link graph (default: 10)\n\n\n--link-mentions\n: Whether to convert entity names into links (default: True)\n\n\n--min-link-prob\n: An entity name is ignored if the probability of the name appearing as a link is less than this value (default: 0.2)\n\n\n--min-prior-prob\n: An entity is not registered as a referent of an entity name if the probability of the entity name referring to the entity is less than this value (default: 0.01)\n\n\n--max-mention-len\n: The maximum number of characters in an entity name (default: 20)\n\n\n--init-alpha\n: The initial learning rate (default: 0.025)\n\n\n--min-alpha\n: The minimum learning rate (default: 0.0001)\n\n\n--sample\n: The parameter that controls the downsampling of frequent words (default: 1e-4)\n\n\n--word-neg-power\n: Negative sampling of words is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0.75)\n\n\n--entity-neg-power\n: Negative sampling of entities is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0)\n\n\n--pool-size\n: The number of worker processes (default: the number of CPUs)\n\n\n\n\nThe \ntrain\n command internally calls the five commands described below (namely, \nbuild_dump_db\n, \nbuild_dictionary\n, \nbuild_link_graph\n, \nbuild_mention_db\n, and \ntrain_embedding\n).\n\n\nBuilding Dump Database\n\uf0c1\n\n\nThe \nbuild_dump_db\n command creates a database that contains Wikipedia pages each of which consists of texts and anchor links in it.\n\n\n% wikipedia2vec build_dump_db DUMP_FILE OUT_FILE\n\n\n\n\nArguments:\n\n\n\n\nDUMP_FILE\n: The Wikipedia dump file\n\n\nOUT_FILE\n: The output file\n\n\n\n\nOptions:\n\n\n\n\n--pool-size\n: The number of worker processes (default: the number of CPUs)\n\n\n\n\nBuilding Dictionary\n\uf0c1\n\n\nThe \nbuild_dictionary\n command builds a dictionary of words and entities.\n\n\n% wikipedia2vec build_dictionary DUMP_DB_FILE OUT_FILE\n\n\n\n\nArguments:\n\n\n\n\nDUMP_DB_FILE\n: The database file generated using the \nbuild_dump_db\n command\n\n\nOUT_FILE\n: The output file\n\n\n\n\nOptions:\n\n\n\n\n--lowercase/--no-lowercase\n: Whether to lowercase words (default: True)\n\n\n--tokenizer\n: The name of the tokenizer used to tokenize a text into words. Possible choices are \nregexp\n, \nicu\n, \nmecab\n, and \njieba\n\n\n--min-word-count\n: A word is ignored if the total frequency of the word is less than this value (default: 10)\n\n\n--min-entity-count\n: An entity is ignored if the total frequency of the entity appearing as the referent of an anchor link is less than this value (default: 5)\n\n\n--min-paragraph-len\n: A paragraph is ignored if its length is shorter than this value (default: 5)\n\n\n--category/--no-category\n: Whether to include Wikipedia categories in the dictionary (default:False)\n\n\n--disambi/--no-disambi\n: Whether to include disambiguation entities in the dictionary (default:False)\n\n\n--pool-size\n: The number of worker processes (default: the number of CPUs)\n\n\n\n\nBuilding Link Graph (Optional)\n\uf0c1\n\n\nThe \nbuild_link_graph\n command generates a sparse matrix representing the link structure between Wikipedia entities.\n\n\n% wikipedia2vec build_link_graph DUMP_DB_FILE DIC_FILE OUT_FILE\n\n\n\n\nArguments:\n\n\n\n\nDUMP_DB_FILE\n: The database file generated using the \nbuild_dump_db\n command\n\n\nDIC_FILE\n: The dictionary file generated by the \nbuild_dictionary\n command\n\n\nOUT_FILE\n: The output file\n\n\n\n\nOptions:\n\n\n\n\n--pool-size\n: The number of worker processes (default: the number of CPUs)\n\n\n\n\nBuilding Mention DB (Optional)\n\uf0c1\n\n\nThe \nbuild_mention_db\n command builds a database that contains the mappings of entity names (mentions) and their possible referent entities.\n\n\n% wikipedia2vec build_mention_db DUMP_DB_FILE DIC_FILE OUT_FILE\n\n\n\n\nArguments:\n\n\n\n\nDUMP_DB_FILE\n: The database file generated using the \nbuild_dump_db\n command\n\n\nDIC_FILE\n: The dictionary file generated by the \nbuild_dictionary\n command\n\n\nOUT_FILE\n: The output file\n\n\n\n\nOptions:\n\n\n\n\n--min-link-prob\n: An entity name is ignored if the probability of the name appearing as a link is less than this value (default: 0.2)\n\n\n--min-prior-prob\n: An entity is not registered as a referent of an entity name if the probability of the entity name referring to the entity is less than this value (default: 0.01)\n\n\n--max-mention-len\n: The maximum number of characters in an entity name (default: 20)\n\n\n--case-sensitive\n: Whether to detect entity names in a case sensitive manner (default: False)\n\n\n--tokenizer\n: The name of the tokenizer used to tokenize a text into words. Possible choices are \nregexp\n, \nicu\n, \nmecab\n, and \njieba\n\n\n--pool-size\n: The number of worker processes (default: the number of CPUs)\n\n\n\n\nLearning Embeddings\n\uf0c1\n\n\nThe \ntrain_embedding\n command runs the training of the embeddings.\n\n\n% wikipedia2vec train_embedding DUMP_DB_FILE DIC_FILE OUT_FILE\n\n\n\n\nArguments:\n\n\n\n\nDUMP_DB_FILE\n: The database file generated using the \nbuild_dump_db\n command\n\n\nDIC_FILE\n: The dictionary file generated by the \nbuild_dictionary\n command\n\n\nOUT_FILE\n: The output file\n\n\n\n\nOptions:\n\n\n\n\n--link-graph\n: The link graph file generated using the \nbuild_link_graph\n command\n\n\n--mention-db\n: The mention DB file generated using the \nbuild_mention_db\n command\n\n\n--dim-size\n: The number of dimensions of the embeddings (default: 100)\n\n\n--window\n: The maximum distance between the target item (word or entity) and the context word to be predicted (default: 5)\n\n\n--iteration\n: The number of iterations for Wikipedia pages (default: 5)\n\n\n--negative\n: The number of negative samples (default: 5)\n\n\n--tokenizer\n: The name of the tokenizer used to tokenize a text into words. Possible values are \nregexp\n, \nicu\n, \nmecab\n, and \njieba\n\n\n--sent-detect\n: The sentence detector used to split texts into sentences. Currently, only \nicu\n is the possible value (default: None)\n\n\n--entities-per-page\n: For processing each page, the specified number of randomly chosen entities are used to predict their neighboring entities in the link graph (default: 10)\n\n\n--init-alpha\n: The initial learning rate (default: 0.025)\n\n\n--min-alpha\n: The minimum learning rate (default: 0.0001)\n\n\n--sample\n: The parameter that controls the downsampling of frequent words (default: 1e-4)\n\n\n--word-neg-power\n: Negative sampling of words is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0.75)\n\n\n--entity-neg-power\n: Negative sampling of entities is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0)\n\n\n--pool-size\n: The number of worker processes (default: the number of CPUs)\n\n\n\n\nSaving Embeddings in Text Format\n\uf0c1\n\n\nsave_text\n outputs a model in a text format.\n\n\n% wikipedia2vec save_text MODEL_FILE OUT_FILE\n\n\n\n\nArguments:\n\n\n\n\nMODEL_FILE\n: The model file generated by the \ntrain_embedding\n command\n\n\nOUT_FILE\n: The output file\n\n\n\n\nOptions:\n\n\n\n\n--out-format\n: The output format. Possible values are \ndefault\n, \nword2vec\n, and \nglove\n. If \nword2vec\n and \nglove\n are specified, the format adopted by \nWord2Vec\n and \nGloVe\n are used, respectively.",
            "title": "Learning Embeddings"
        },
        {
            "location": "/commands/#learning-embeddings",
            "text": "First, you need to download a source Wikipedia dump file (e.g., enwiki-latest-pages-articles.xml.bz2) from  Wikimedia Downloads .\nThe English dump file can be obtained by running the following command.  % wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2  Note that you do not need to decompress the dump file.  Then, the embeddings can be trained from a Wikipedia dump using the  train  command.  % wikipedia2vec train DUMP_FILE OUT_FILE  Arguments:   DUMP_FILE : The Wikipedia dump file  OUT_FILE : The output file   Options:   --dim-size : The number of dimensions of the embeddings (default: 100)  --window : The maximum distance between the target item (word or entity) and the context word to be predicted (default: 5)  --iteration : The number of iterations for Wikipedia pages (default: 5)  --negative : The number of negative samples (default: 5)  --lowercase/--no-lowercase : Whether to lowercase words (default: True)  --tokenizer : The name of the tokenizer used to tokenize a text into words. Possible choices are  regexp ,  icu ,  mecab , and  jieba  --sent-detect : The sentence detector used to split texts into sentences. Currently, only  icu  is the possible value (default: None)  --min-word-count : A word is ignored if the total frequency of the word is less than this value (default: 10)  --min-entity-count : An entity is ignored if the total frequency of the entity appearing as the referent of an anchor link is less than this value (default: 5)  --min-paragraph-len : A paragraph is ignored if its length is shorter than this value (default: 5)  --category/--no-category : Whether to include Wikipedia categories in the dictionary (default:False)  --disambi/--no-disambi : Whether to include disambiguation entities in the dictionary (default:False)  --link-graph/--no-link-graph : Whether to learn from the Wikipedia link graph (default: True)  --entities-per-page : For processing each page, the specified number of randomly chosen entities are used to predict their neighboring entities in the link graph (default: 10)  --link-mentions : Whether to convert entity names into links (default: True)  --min-link-prob : An entity name is ignored if the probability of the name appearing as a link is less than this value (default: 0.2)  --min-prior-prob : An entity is not registered as a referent of an entity name if the probability of the entity name referring to the entity is less than this value (default: 0.01)  --max-mention-len : The maximum number of characters in an entity name (default: 20)  --init-alpha : The initial learning rate (default: 0.025)  --min-alpha : The minimum learning rate (default: 0.0001)  --sample : The parameter that controls the downsampling of frequent words (default: 1e-4)  --word-neg-power : Negative sampling of words is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0.75)  --entity-neg-power : Negative sampling of entities is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0)  --pool-size : The number of worker processes (default: the number of CPUs)   The  train  command internally calls the five commands described below (namely,  build_dump_db ,  build_dictionary ,  build_link_graph ,  build_mention_db , and  train_embedding ).",
            "title": "Learning Embeddings"
        },
        {
            "location": "/commands/#building-dump-database",
            "text": "The  build_dump_db  command creates a database that contains Wikipedia pages each of which consists of texts and anchor links in it.  % wikipedia2vec build_dump_db DUMP_FILE OUT_FILE  Arguments:   DUMP_FILE : The Wikipedia dump file  OUT_FILE : The output file   Options:   --pool-size : The number of worker processes (default: the number of CPUs)",
            "title": "Building Dump Database"
        },
        {
            "location": "/commands/#building-dictionary",
            "text": "The  build_dictionary  command builds a dictionary of words and entities.  % wikipedia2vec build_dictionary DUMP_DB_FILE OUT_FILE  Arguments:   DUMP_DB_FILE : The database file generated using the  build_dump_db  command  OUT_FILE : The output file   Options:   --lowercase/--no-lowercase : Whether to lowercase words (default: True)  --tokenizer : The name of the tokenizer used to tokenize a text into words. Possible choices are  regexp ,  icu ,  mecab , and  jieba  --min-word-count : A word is ignored if the total frequency of the word is less than this value (default: 10)  --min-entity-count : An entity is ignored if the total frequency of the entity appearing as the referent of an anchor link is less than this value (default: 5)  --min-paragraph-len : A paragraph is ignored if its length is shorter than this value (default: 5)  --category/--no-category : Whether to include Wikipedia categories in the dictionary (default:False)  --disambi/--no-disambi : Whether to include disambiguation entities in the dictionary (default:False)  --pool-size : The number of worker processes (default: the number of CPUs)",
            "title": "Building Dictionary"
        },
        {
            "location": "/commands/#building-link-graph-optional",
            "text": "The  build_link_graph  command generates a sparse matrix representing the link structure between Wikipedia entities.  % wikipedia2vec build_link_graph DUMP_DB_FILE DIC_FILE OUT_FILE  Arguments:   DUMP_DB_FILE : The database file generated using the  build_dump_db  command  DIC_FILE : The dictionary file generated by the  build_dictionary  command  OUT_FILE : The output file   Options:   --pool-size : The number of worker processes (default: the number of CPUs)",
            "title": "Building Link Graph (Optional)"
        },
        {
            "location": "/commands/#building-mention-db-optional",
            "text": "The  build_mention_db  command builds a database that contains the mappings of entity names (mentions) and their possible referent entities.  % wikipedia2vec build_mention_db DUMP_DB_FILE DIC_FILE OUT_FILE  Arguments:   DUMP_DB_FILE : The database file generated using the  build_dump_db  command  DIC_FILE : The dictionary file generated by the  build_dictionary  command  OUT_FILE : The output file   Options:   --min-link-prob : An entity name is ignored if the probability of the name appearing as a link is less than this value (default: 0.2)  --min-prior-prob : An entity is not registered as a referent of an entity name if the probability of the entity name referring to the entity is less than this value (default: 0.01)  --max-mention-len : The maximum number of characters in an entity name (default: 20)  --case-sensitive : Whether to detect entity names in a case sensitive manner (default: False)  --tokenizer : The name of the tokenizer used to tokenize a text into words. Possible choices are  regexp ,  icu ,  mecab , and  jieba  --pool-size : The number of worker processes (default: the number of CPUs)",
            "title": "Building Mention DB (Optional)"
        },
        {
            "location": "/commands/#learning-embeddings_1",
            "text": "The  train_embedding  command runs the training of the embeddings.  % wikipedia2vec train_embedding DUMP_DB_FILE DIC_FILE OUT_FILE  Arguments:   DUMP_DB_FILE : The database file generated using the  build_dump_db  command  DIC_FILE : The dictionary file generated by the  build_dictionary  command  OUT_FILE : The output file   Options:   --link-graph : The link graph file generated using the  build_link_graph  command  --mention-db : The mention DB file generated using the  build_mention_db  command  --dim-size : The number of dimensions of the embeddings (default: 100)  --window : The maximum distance between the target item (word or entity) and the context word to be predicted (default: 5)  --iteration : The number of iterations for Wikipedia pages (default: 5)  --negative : The number of negative samples (default: 5)  --tokenizer : The name of the tokenizer used to tokenize a text into words. Possible values are  regexp ,  icu ,  mecab , and  jieba  --sent-detect : The sentence detector used to split texts into sentences. Currently, only  icu  is the possible value (default: None)  --entities-per-page : For processing each page, the specified number of randomly chosen entities are used to predict their neighboring entities in the link graph (default: 10)  --init-alpha : The initial learning rate (default: 0.025)  --min-alpha : The minimum learning rate (default: 0.0001)  --sample : The parameter that controls the downsampling of frequent words (default: 1e-4)  --word-neg-power : Negative sampling of words is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0.75)  --entity-neg-power : Negative sampling of entities is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0)  --pool-size : The number of worker processes (default: the number of CPUs)",
            "title": "Learning Embeddings"
        },
        {
            "location": "/commands/#saving-embeddings-in-text-format",
            "text": "save_text  outputs a model in a text format.  % wikipedia2vec save_text MODEL_FILE OUT_FILE  Arguments:   MODEL_FILE : The model file generated by the  train_embedding  command  OUT_FILE : The output file   Options:   --out-format : The output format. Possible values are  default ,  word2vec , and  glove . If  word2vec  and  glove  are specified, the format adopted by  Word2Vec  and  GloVe  are used, respectively.",
            "title": "Saving Embeddings in Text Format"
        },
        {
            "location": "/usage/",
            "text": "Basic Usage\n\uf0c1\n\n\n\n\n>>> from wikipedia2vec import Wikipedia2Vec\n\n>>> wiki2vec = Wikipedia2Vec.load(MODEL_FILE)\n\n>>> wiki2vec.get_word_vector('the')\nmemmap([ 0.01617998, -0.03325786, -0.01397999, -0.00150471,  0.03237337,\n...\n       -0.04226106, -0.19677088, -0.31087297,  0.1071524 , -0.09824426], dtype=float32)\n\n>>> wiki2vec.get_entity_vector('Scarlett Johansson')\nmemmap([-0.19793572,  0.30861306,  0.29620451, -0.01193621,  0.18228433,\n...\n        0.04986198,  0.24383858, -0.01466644,  0.10835337, -0.0697331 ], dtype=float32)\n\n>>> wiki2vec.most_similar(wiki2vec.get_word('yoda'), 5)\n[(<Word yoda>, 1.0),\n (<Entity Yoda>, 0.84333622),\n (<Word darth>, 0.73328167),\n (<Word kenobi>, 0.7328127),\n (<Word jedi>, 0.7223742)]\n\n>>> wiki2vec.most_similar(wiki2vec.get_entity('Scarlett Johansson'), 5)\n[(<Entity Scarlett Johansson>, 1.0),\n (<Entity Natalie Portman>, 0.75090045),\n (<Entity Eva Mendes>, 0.73651594),\n (<Entity Emma Stone>, 0.72868186),\n (<Entity Cameron Diaz>, 0.72390842)]",
            "title": "Basic Usage"
        },
        {
            "location": "/usage/#basic-usage",
            "text": ">>> from wikipedia2vec import Wikipedia2Vec\n\n>>> wiki2vec = Wikipedia2Vec.load(MODEL_FILE)\n\n>>> wiki2vec.get_word_vector('the')\nmemmap([ 0.01617998, -0.03325786, -0.01397999, -0.00150471,  0.03237337,\n...\n       -0.04226106, -0.19677088, -0.31087297,  0.1071524 , -0.09824426], dtype=float32)\n\n>>> wiki2vec.get_entity_vector('Scarlett Johansson')\nmemmap([-0.19793572,  0.30861306,  0.29620451, -0.01193621,  0.18228433,\n...\n        0.04986198,  0.24383858, -0.01466644,  0.10835337, -0.0697331 ], dtype=float32)\n\n>>> wiki2vec.most_similar(wiki2vec.get_word('yoda'), 5)\n[(<Word yoda>, 1.0),\n (<Entity Yoda>, 0.84333622),\n (<Word darth>, 0.73328167),\n (<Word kenobi>, 0.7328127),\n (<Word jedi>, 0.7223742)]\n\n>>> wiki2vec.most_similar(wiki2vec.get_entity('Scarlett Johansson'), 5)\n[(<Entity Scarlett Johansson>, 1.0),\n (<Entity Natalie Portman>, 0.75090045),\n (<Entity Eva Mendes>, 0.73651594),\n (<Entity Emma Stone>, 0.72868186),\n (<Entity Cameron Diaz>, 0.72390842)]",
            "title": "Basic Usage"
        },
        {
            "location": "/pretrained/",
            "text": "Pretrained Embeddings\n\uf0c1\n\n\n\n\nWe provide pretrained embeddings for 12 languages in binary and text format.\nThe binary files can be loaded using the \nWikipedia2Vec.load()\n method (see \nBasic Usage\n).\nThe text files are compatible with the text format of \nWord2vec\n.\nTherefore, these files can be loaded using other libraries such as Gensim's \nload_word2vec_format()\n.\nIn the text files, all entities have a prefix \nENTITY/\n to distinguish them from words.\nPlease \nuncompress the files\n before using them.\n\n\nEnglish\n\uf0c1\n\n\n\n\nenwiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n  \n500d (bin)\n\n  \n500d (txt)\n\n\nenwiki_20180420_nolg\n (window=5, iteration=10, negative=15, no link graph):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n  \n500d (bin)\n\n  \n500d (txt)\n\n\nenwiki_20180420_win10\n (window=10, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n  \n500d (bin)\n\n  \n500d (txt)\n\n\n\n\nArabic\n\uf0c1\n\n\n\n\narwiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n\n\n\nChinese\n\uf0c1\n\n\n\n\nzhwiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n\n\n\nDutch\n\uf0c1\n\n\n\n\nnlwiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n\n\n\nFrench\n\uf0c1\n\n\n\n\nfrwiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n\n\n\nGerman\n\uf0c1\n\n\n\n\ndewiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n\n\n\nItalian\n\uf0c1\n\n\n\n\nitwiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n\n\n\nJapanese\n\uf0c1\n\n\n\n\njawiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n\n\n\nPolish\n\uf0c1\n\n\n\n\nplwiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n\n\n\nPortuguese\n\uf0c1\n\n\n\n\nptwiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n\n\n\nRussian\n\uf0c1\n\n\n\n\nruwiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)\n\n\n\n\nSpanish\n\uf0c1\n\n\n\n\neswiki_20180420\n (window=5, iteration=10, negative=15):\n  \n100d (bin)\n\n  \n100d (txt)\n\n  \n300d (bin)\n\n  \n300d (txt)",
            "title": "Pretrained Embeddings"
        },
        {
            "location": "/pretrained/#pretrained-embeddings",
            "text": "We provide pretrained embeddings for 12 languages in binary and text format.\nThe binary files can be loaded using the  Wikipedia2Vec.load()  method (see  Basic Usage ).\nThe text files are compatible with the text format of  Word2vec .\nTherefore, these files can be loaded using other libraries such as Gensim's  load_word2vec_format() .\nIn the text files, all entities have a prefix  ENTITY/  to distinguish them from words.\nPlease  uncompress the files  before using them.",
            "title": "Pretrained Embeddings"
        },
        {
            "location": "/pretrained/#english",
            "text": "enwiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt) \n   500d (bin) \n   500d (txt)  enwiki_20180420_nolg  (window=5, iteration=10, negative=15, no link graph):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt) \n   500d (bin) \n   500d (txt)  enwiki_20180420_win10  (window=10, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt) \n   500d (bin) \n   500d (txt)",
            "title": "English"
        },
        {
            "location": "/pretrained/#arabic",
            "text": "arwiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt)",
            "title": "Arabic"
        },
        {
            "location": "/pretrained/#chinese",
            "text": "zhwiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt)",
            "title": "Chinese"
        },
        {
            "location": "/pretrained/#dutch",
            "text": "nlwiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt)",
            "title": "Dutch"
        },
        {
            "location": "/pretrained/#french",
            "text": "frwiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt)",
            "title": "French"
        },
        {
            "location": "/pretrained/#german",
            "text": "dewiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt)",
            "title": "German"
        },
        {
            "location": "/pretrained/#italian",
            "text": "itwiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt)",
            "title": "Italian"
        },
        {
            "location": "/pretrained/#japanese",
            "text": "jawiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt)",
            "title": "Japanese"
        },
        {
            "location": "/pretrained/#polish",
            "text": "plwiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt)",
            "title": "Polish"
        },
        {
            "location": "/pretrained/#portuguese",
            "text": "ptwiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt)",
            "title": "Portuguese"
        },
        {
            "location": "/pretrained/#russian",
            "text": "ruwiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt)",
            "title": "Russian"
        },
        {
            "location": "/pretrained/#spanish",
            "text": "eswiki_20180420  (window=5, iteration=10, negative=15):\n   100d (bin) \n   100d (txt) \n   300d (bin) \n   300d (txt)",
            "title": "Spanish"
        },
        {
            "location": "/benchmarks/",
            "text": "Benchmarks\n\uf0c1\n\n\n\n\nWe provide the performance on a variety of benchmarks for the Wikipedia2Vec pretrained model.\n\n\nEvaluations are conducted end-to-end, and we used  \nintrinsic_eval.py\n to evaluate Wikipedia2Vec performance.\n\n\nAbout The Evaluations\n\uf0c1\n\n\nWe conducted evaluations on a variety of intrinsic tasks.\n\n\nWikipedia2Vec learns embeddings that map words and entities into a unified continuous vector space.\nTherefore, in this experiment, we evaluated the learned embeddings of words and those of entities separately.\nIn particular, we evaluated the word embeddings with \nWord Similarity\n and \nWord Analogy\n, while we evaluated the entity embeddings with \nEntity Relatedness\n.\n\n\nWord Similarity\n\uf0c1\n\n\nWord Similarity is a task for intrinsic evaluation of word embeddings, which correlates the distance between vectors and human judgments of semantic similarity.\n\n\n\n\nMEN-TR-3k\n (\nBruni et al.,2014\n)\n\n\nRG-65\n\n(\nRubenstein et al., 1965\n)\n\n\nSimLex999\n (\nHill et al, 2014\n)\n\n\nWS-353-REL\n (\nFinkelstein et al., 2002\n)\n\n\nWS-353-SIM\n (\nFinkelstein et al., 2002\n)\n\n\n\n\nWord Analogy\n\uf0c1\n\n\nWord Analogy is the task, which inspects syntactic, morphosyntactic and semantic properties of words and phrases.\n\n\n\n\nGOOGLE ANALOGY (Syntactic)\n (\nMikolov et al., 2013\n)\n\n\nGOOGLE ANALOGY (Semantic)\n (\nMikolov et al., 2013\n)\n\n\n\n\nEntity Relatedness\n\uf0c1\n\n\nEntity Relatedness is the intrinsic evaluation task for entities, where the quality of entity embeddings is evaluated using the human ratings of relatedness between entities.\n\n\n\n\nKORE\n (\nHoffart et al., 2012\n)\n\n\n\n\nModel Comparison with Gensim\n\uf0c1\n\n\nIn this section, we compare the performance on Word Similarity and Word Analogy\nbenchmarks of our Wikipedia2Vec-trained model\nwith the model trained by \ngensim\n.\nWe do not compare the performance on Entity Relatedness here.\n\n\nFor both embeddings, we set the \nwindow size\n to 5, \niteration\n to 10, and\n\nnegative sampling count\n to 15.\nFor training, we only used English Wikipedia dump, without adding any additional large-scale corpora.\nWe used  \ngensim_wikipedia.py\n to train gensim-based word embeddings.\n\n\nThe results on a variety of benckmarks show that Wikipedia2Vec pretrained embeddings\n(\nenwiki_20180420_300d.pkl\n) outperformed gensim pretrained embeddings.\n\n\n\n\n\nWord Similarity\n\uf0c1\n\n\nWe evaluated the performance on 6 Word Similarity benchmarks,\nand our Wikipedia2Vec pretrained model outperformed gensim pretrained in almost all of\nthe benchmarks.\n\n\n\n\n\n\n\n\nDataset\n\n\nWikipedia2Vec\n\n\ngensim\n\n\n\n\n\n\n\n\n\n\nMEN-TR-3k\n\n\n0.749\n\n\n0.7315\n\n\n\n\n\n\nRG-65\n\n\n0.7837\n\n\n0.7582\n\n\n\n\n\n\nSimLex999\n\n\n0.3815\n\n\n0.3471\n\n\n\n\n\n\nWS-353-ALL\n\n\n0.6952\n\n\n0.6933\n\n\n\n\n\n\nWS-353-REL\n\n\n0.6233\n\n\n0.625\n\n\n\n\n\n\nWS-353-SIM\n\n\n0.7597\n\n\n0.7833\n\n\n\n\n\n\n\n\nWord Analogy\n\uf0c1\n\n\nIn both of the two Word Analogy tasks, the embeddings trained by Wikipedia2Vec\nsignificantly outperformed the embeddings trained by gensim.\n\n\n\n\n\n\n\n\nDataset\n\n\nWikipedia2Vec\n\n\ngensim\n\n\n\n\n\n\n\n\n\n\nGOOGLE ANALOGY (Semantic)\n\n\n0.7892\n\n\n0.782\n\n\n\n\n\n\nGOOGLE ANALOGY (Syntactic)\n\n\n0.6812\n\n\n0.5783\n\n\n\n\n\n\n\n\nModel Comparison with word2vec, GloVe\n\uf0c1\n\n\nIn this section, we compare the performance of\n\nword2vec\n and \nGloVe\n pretrained embeddings\nand our Wikipedia2Vec word embeddings.\n\n\nIn the previous section, we compared the models only trained with English\nWikipedia dump.\nIt is widely known that the quality of the word embeddings increases significantly\nwith large amount of the training data.\nTherefore, we compare the performances of publicly available\nword embeddings trained with much larger amount of training data.\n\n\nWe use \nword2vec google_news pretrained embedding\n\n(100B words, 3M vocab),\nGloVe's \nglove.42B.300d\n\n(42B tokens, 1.9M vocab) and \nglove.840B.300d\n\n(840B tokens, 2.2M vocab) pretrained embeddings.\n\n\nWord Similarity\n\uf0c1\n\n\nThe glove.840B.300d outperformed our embeddings trained with Wikipedia on\nall of the benchmarks, benefiting from its huge vocabulary size and Common Crawl-based\nhuge training corpus.\n\n\nOn the other hand, our Wikipedia2Vec pretrained embeddings outperformed word2vec_gnews and\nglove_42b_300d on some of the benchmarks,\neven though training corpora for these embeddings are three or four orders of\nmagnitudes larger than the training corpus obtained only from Wikipedia.\n\n\n\n\n\n\n\n\nDataset\n\n\nWikipedia2Vec\n\n\nword2vec_gnews\n\n\nglove_42b_300d\n\n\nglove_840b_300d\n\n\n\n\n\n\n\n\n\n\nMEN-TR-3k\n\n\n0.749\n\n\nOOV\n\n\n0.7362\n\n\n0.8016\n\n\n\n\n\n\nRG-65\n\n\n0.7837\n\n\n0.7608\n\n\n0.8171\n\n\n0.7696\n\n\n\n\n\n\nSimLex999\n\n\n0.3815\n\n\n0.442\n\n\n0.3738\n\n\n0.4083\n\n\n\n\n\n\nWS-353-ALL\n\n\n0.6952\n\n\n0.7\n\n\n0.6321\n\n\n0.7379\n\n\n\n\n\n\nWS-353-REL\n\n\n0.6233\n\n\n0.6355\n\n\n0.5706\n\n\n0.6876\n\n\n\n\n\n\nWS-353-SIM\n\n\n0.7597\n\n\n0.7717\n\n\n0.6979\n\n\n0.8031\n\n\n\n\n\n\n\n\nWord Analogy\n\uf0c1\n\n\nIn Word Analogy evaluations, we found the same trend as Word Similarity,\nand Wikipedia2Vec embeddings shows competitive performance despite of its smaller scale training corpus.\nWe excluded word2vec_gnews here because many of the words are missing in both of the datasets.\n\n\n\n\n\n\n\n\nDataset\n\n\nWikipedia2Vec\n\n\nglove_42b_300d\n\n\nglove_42b_300d\n\n\n\n\n\n\n\n\n\n\nGOOGLE ANALOGY (Semantic)\n\n\n0.7892\n\n\n0.8185\n\n\n0.794\n\n\n\n\n\n\nGOOGLE ANALOGY (Syntactic)\n\n\n0.6812\n\n\n0.6925\n\n\n0.7567\n\n\n\n\n\n\n\n\nComparison with State of The Art Entity Embeddings Method\n\uf0c1\n\n\nRistoski et.al\n proposed \nRDF2Vec\n, an approach that learns entity embeddings using word embedding methods (i.e., CBOW and skip-gram) with RDF graphs as inputs.\n\n\nRDF2Vec model achieved the state of the art performance on KORE dataset.\nWe compare Entity Relatedness performance of RDF2Vec and Wikipedia2Vec with the same number of word dimensions.\n\n\nAs shown in the table below, except for the only one category (i.e., Hollywood celebrities)\nWikipedia2Vec achieved higher performance on KORE dataset.\n\n\n\n\n\n\n\n\nCategory\n\n\nWikipedia2Vec\n\n\nRDF2Vec (\nRistoski et.al\n)\n\n\n\n\n\n\n\n\n\n\nIT companies\n\n\n0.7934\n\n\n0.743\n\n\n\n\n\n\nHollywood Celebrities\n\n\n0.6887\n\n\n0.734\n\n\n\n\n\n\nTelevision Series\n\n\n0.6415\n\n\n0.635\n\n\n\n\n\n\nVideo Games\n\n\n0.7261\n\n\n0.669\n\n\n\n\n\n\nChuck Norris\n\n\n0.6286\n\n\n0.628\n\n\n\n\n\n\nAll\n\n\n0.7084\n\n\n0.692\n\n\n\n\n\n\n\n\nThe Effects of Parameter Tuning\n\uf0c1\n\n\nWe also provide benchmark evaluations of Wikipedia2Vec pretrained models,\nwith different training settings to show how the performance varies on various hyper-parameters.\nAll of the pretrained models are available, and you can download them from the\n\npretrained embeddings page\n.\n\n\nLink Graph\n\uf0c1\n\n\nThe link graph model that learns to estimate neighboring entities given an entity\n in the link graph of Wikipedia entities.\nWe compared the performance of the link graph model with the no link graph model to\nsee the effectiveness of the link graphs between entities.\nExcept for the link graph, we set all of the parameters to the same.\nFor both embeddings, we set the \nwindow size\n to 5, \niteration\n to 10, and\n\nnegative sampling count\n to 15.\n\n\nWord Similarity\n\uf0c1\n\n\nIn terms of the performance on Word Similarity task, no link graph model\noutperformed the link graph model.\n\n\n\n\n\n\n\n\nDataset\n\n\nWikipedia2Vec\n\n\nWikipedia2Vec (no link graph)\n\n\n\n\n\n\n\n\n\n\nMEN-TR-3k\n\n\n0.749\n\n\n0.7467\n\n\n\n\n\n\nRG-65\n\n\n0.7837\n\n\n0.7987\n\n\n\n\n\n\nSimLex999\n\n\n0.3815\n\n\n0.3867\n\n\n\n\n\n\nWS-353-ALL\n\n\n0.6952\n\n\n0.7009\n\n\n\n\n\n\nWS-353-REL\n\n\n0.6233\n\n\n0.6304\n\n\n\n\n\n\nWS-353-SIM\n\n\n0.7597\n\n\n0.7643\n\n\n\n\n\n\n\n\nWord Analogy\n\uf0c1\n\n\nThe link graph model achieves higher performance on both of the Word Analogy task.\n\n\n\n\n\n\n\n\nDataset\n\n\nWikipedia2Vec\n\n\nWikipedia2Vec (no link graph)\n\n\n\n\n\n\n\n\n\n\nGOOGLE ANALOGY (Semantic)\n\n\n0.7892\n\n\n0.7804\n\n\n\n\n\n\nGOOGLE ANALOGY (Syntactic)\n\n\n0.6812\n\n\n0.6703\n\n\n\n\n\n\n\n\nEntity Relatedness\n\uf0c1\n\n\nUnsurprisingly, without link graph, the model shows significantly huge drop in its\nperformance in Entity Relatedness tasks, because of the lacking of the information\nabout the entity relationship.\n\n\n\n\n\n\n\n\nDataset\n\n\nWikipedia2Vec\n\n\nWikipedia2Vec (no link graph)\n\n\n\n\n\n\n\n\n\n\nKORE\n\n\n0.6905\n\n\n0.5892\n\n\n\n\n\n\n\n\nWindow Size\n\uf0c1\n\n\nPrevious work shows that the window size for word embeddings training does matter.\nWe conducted evaluation on Wikipedia2Vec models with different window sizes,\nto see how important the window size is for the performance on intrinsic evaluation tasks.\n\n\nWe compare the performance of \nenwiki_20180420 (300d)\n (Wikipedia2Vec (window=5)) with the one of\n\nenwiki_20180420_win10 (300d)\n (Wikipedia2Vec (window=10)).\n\n\nFor both embeddings, we set the \niteration\n to 10, and\n\nnegative sampling count\n to 15.\nFor training, we only use English Wikipedia dump, without adding any additional large-scale corpora.\n\n\nWord Similarity\n\uf0c1\n\n\nOur experimental results show that larger window size seems to improve the\nperformance on Word Similarity, on the all benchmarks except SimLex999.\n\n\nIt should be noted that \nChiu et al.\n showed\nthat the performance on word similarity benchmarks like MEN-TR-3k have negative correlations with downstream tasks, while only SimLex999 has high correlation with extrinsic measures.\nThey also observed that decreasing window size improves the performance on downstream tasks, while it leads to deterioration of performance on most of the Word Similarity benchmarks.\n\n\n\n\n\n\n\n\nDataset\n\n\nWikipedia2Vec (window=5)\n\n\nWikipedia2Vec (window=10)\n\n\n\n\n\n\n\n\n\n\nMEN-TR-3k\n\n\n0.749\n\n\n0.7541\n\n\n\n\n\n\nRG-65\n\n\n0.7837\n\n\n0.7861\n\n\n\n\n\n\nSimLex999\n\n\n0.3815\n\n\n0.3578\n\n\n\n\n\n\nWS-353-ALL\n\n\n0.6952\n\n\n0.71\n\n\n\n\n\n\nWS-353-REL\n\n\n0.6233\n\n\n0.6435\n\n\n\n\n\n\nWS-353-SIM\n\n\n0.7597\n\n\n0.7848\n\n\n\n\n\n\n\n\nWord Analogy\n\uf0c1\n\n\n\n\n\n\n\n\nDataset\n\n\nWikipedia2Vec (window=5)\n\n\nWikipedia2Vec (window=10)\n\n\n\n\n\n\n\n\n\n\nGOOGLE ANALOGY (Semantic)\n\n\n0.7892\n\n\n0.789\n\n\n\n\n\n\nGOOGLE ANALOGY (Syntactic)\n\n\n0.6812\n\n\n0.6529\n\n\n\n\n\n\n\n\nEntity Relatedness\n\uf0c1\n\n\n\n\n\n\n\n\nDataset\n\n\nWikipedia2Vec (window=5)\n\n\nWikipedia2Vec (window=10)\n\n\n\n\n\n\n\n\n\n\nKORE\n\n\n0.6905\n\n\n0.6811\n\n\n\n\n\n\n\n\nMultilingual Evaluation\n\uf0c1\n\n\nWe evaluate our word embeddings (300d) on German, French, Spanish, Portuguese, Polish and Chinese\nword analogy task.\n\n\nMultilingual Word Analogy Dataset\n\uf0c1\n\n\nWe use the same word analogy dataset as \nGrave et.al\n.\n\n\n\n\nGoogle Analogy Dataset (German)\n\n\nGoogle Analogy Dataset (French)\n\n\nGoogle Analogy Dataset (Spanish)\n\n\nGoogle Analogy Dataset (Portuguese)\n\n\nGoogle Analogy Dataset (Polish)\n\n\nGoogle Analogy Dataset (Chinese)\n\n\n\n\nMultilingual Word Analogy Results\n\uf0c1\n\n\nThe results of word analogy task evaluation are as follows.\nFollowing \nGrave et.al\n, we restrict the vocabulary for the analogy tasks to the 200,000 most frequent words from the training data.\n\n\n\n\n\n\n\n\nlanguage\n\n\nWikipedia2Vec\n\n\nfastText (\nGrave et.al\n)\n\n\n\n\n\n\n\n\n\n\nGerman\n\n\n0.617\n\n\n0.61\n\n\n\n\n\n\nFrench\n\n\n0.68\n\n\n0.642\n\n\n\n\n\n\nSpanish\n\n\n0.574\n\n\n0.574\n\n\n\n\n\n\nPortuguese\n\n\n0.53\n\n\n0.54\n\n\n\n\n\n\nPolish\n\n\n0.516\n\n\n0.534\n\n\n\n\n\n\nChinese\n\n\n0.572\n\n\n0.631",
            "title": "Benchmarks"
        },
        {
            "location": "/benchmarks/#benchmarks",
            "text": "We provide the performance on a variety of benchmarks for the Wikipedia2Vec pretrained model.  Evaluations are conducted end-to-end, and we used   intrinsic_eval.py  to evaluate Wikipedia2Vec performance.",
            "title": "Benchmarks"
        },
        {
            "location": "/benchmarks/#about-the-evaluations",
            "text": "We conducted evaluations on a variety of intrinsic tasks.  Wikipedia2Vec learns embeddings that map words and entities into a unified continuous vector space.\nTherefore, in this experiment, we evaluated the learned embeddings of words and those of entities separately.\nIn particular, we evaluated the word embeddings with  Word Similarity  and  Word Analogy , while we evaluated the entity embeddings with  Entity Relatedness .",
            "title": "About The Evaluations"
        },
        {
            "location": "/benchmarks/#word-similarity",
            "text": "Word Similarity is a task for intrinsic evaluation of word embeddings, which correlates the distance between vectors and human judgments of semantic similarity.   MEN-TR-3k  ( Bruni et al.,2014 )  RG-65 \n( Rubenstein et al., 1965 )  SimLex999  ( Hill et al, 2014 )  WS-353-REL  ( Finkelstein et al., 2002 )  WS-353-SIM  ( Finkelstein et al., 2002 )",
            "title": "Word Similarity"
        },
        {
            "location": "/benchmarks/#word-analogy",
            "text": "Word Analogy is the task, which inspects syntactic, morphosyntactic and semantic properties of words and phrases.   GOOGLE ANALOGY (Syntactic)  ( Mikolov et al., 2013 )  GOOGLE ANALOGY (Semantic)  ( Mikolov et al., 2013 )",
            "title": "Word Analogy"
        },
        {
            "location": "/benchmarks/#entity-relatedness",
            "text": "Entity Relatedness is the intrinsic evaluation task for entities, where the quality of entity embeddings is evaluated using the human ratings of relatedness between entities.   KORE  ( Hoffart et al., 2012 )",
            "title": "Entity Relatedness"
        },
        {
            "location": "/benchmarks/#model-comparison-with-gensim",
            "text": "In this section, we compare the performance on Word Similarity and Word Analogy\nbenchmarks of our Wikipedia2Vec-trained model\nwith the model trained by  gensim .\nWe do not compare the performance on Entity Relatedness here.  For both embeddings, we set the  window size  to 5,  iteration  to 10, and negative sampling count  to 15.\nFor training, we only used English Wikipedia dump, without adding any additional large-scale corpora.\nWe used   gensim_wikipedia.py  to train gensim-based word embeddings.  The results on a variety of benckmarks show that Wikipedia2Vec pretrained embeddings\n( enwiki_20180420_300d.pkl ) outperformed gensim pretrained embeddings.",
            "title": "Model Comparison with Gensim"
        },
        {
            "location": "/benchmarks/#word-similarity_1",
            "text": "We evaluated the performance on 6 Word Similarity benchmarks,\nand our Wikipedia2Vec pretrained model outperformed gensim pretrained in almost all of\nthe benchmarks.     Dataset  Wikipedia2Vec  gensim      MEN-TR-3k  0.749  0.7315    RG-65  0.7837  0.7582    SimLex999  0.3815  0.3471    WS-353-ALL  0.6952  0.6933    WS-353-REL  0.6233  0.625    WS-353-SIM  0.7597  0.7833",
            "title": "Word Similarity"
        },
        {
            "location": "/benchmarks/#word-analogy_1",
            "text": "In both of the two Word Analogy tasks, the embeddings trained by Wikipedia2Vec\nsignificantly outperformed the embeddings trained by gensim.     Dataset  Wikipedia2Vec  gensim      GOOGLE ANALOGY (Semantic)  0.7892  0.782    GOOGLE ANALOGY (Syntactic)  0.6812  0.5783",
            "title": "Word Analogy"
        },
        {
            "location": "/benchmarks/#model-comparison-with-word2vec-glove",
            "text": "In this section, we compare the performance of word2vec  and  GloVe  pretrained embeddings\nand our Wikipedia2Vec word embeddings.  In the previous section, we compared the models only trained with English\nWikipedia dump.\nIt is widely known that the quality of the word embeddings increases significantly\nwith large amount of the training data.\nTherefore, we compare the performances of publicly available\nword embeddings trained with much larger amount of training data.  We use  word2vec google_news pretrained embedding \n(100B words, 3M vocab),\nGloVe's  glove.42B.300d \n(42B tokens, 1.9M vocab) and  glove.840B.300d \n(840B tokens, 2.2M vocab) pretrained embeddings.",
            "title": "Model Comparison with word2vec, GloVe"
        },
        {
            "location": "/benchmarks/#word-similarity_2",
            "text": "The glove.840B.300d outperformed our embeddings trained with Wikipedia on\nall of the benchmarks, benefiting from its huge vocabulary size and Common Crawl-based\nhuge training corpus.  On the other hand, our Wikipedia2Vec pretrained embeddings outperformed word2vec_gnews and\nglove_42b_300d on some of the benchmarks,\neven though training corpora for these embeddings are three or four orders of\nmagnitudes larger than the training corpus obtained only from Wikipedia.     Dataset  Wikipedia2Vec  word2vec_gnews  glove_42b_300d  glove_840b_300d      MEN-TR-3k  0.749  OOV  0.7362  0.8016    RG-65  0.7837  0.7608  0.8171  0.7696    SimLex999  0.3815  0.442  0.3738  0.4083    WS-353-ALL  0.6952  0.7  0.6321  0.7379    WS-353-REL  0.6233  0.6355  0.5706  0.6876    WS-353-SIM  0.7597  0.7717  0.6979  0.8031",
            "title": "Word Similarity"
        },
        {
            "location": "/benchmarks/#word-analogy_2",
            "text": "In Word Analogy evaluations, we found the same trend as Word Similarity,\nand Wikipedia2Vec embeddings shows competitive performance despite of its smaller scale training corpus.\nWe excluded word2vec_gnews here because many of the words are missing in both of the datasets.     Dataset  Wikipedia2Vec  glove_42b_300d  glove_42b_300d      GOOGLE ANALOGY (Semantic)  0.7892  0.8185  0.794    GOOGLE ANALOGY (Syntactic)  0.6812  0.6925  0.7567",
            "title": "Word Analogy"
        },
        {
            "location": "/benchmarks/#comparison-with-state-of-the-art-entity-embeddings-method",
            "text": "Ristoski et.al  proposed  RDF2Vec , an approach that learns entity embeddings using word embedding methods (i.e., CBOW and skip-gram) with RDF graphs as inputs.  RDF2Vec model achieved the state of the art performance on KORE dataset.\nWe compare Entity Relatedness performance of RDF2Vec and Wikipedia2Vec with the same number of word dimensions.  As shown in the table below, except for the only one category (i.e., Hollywood celebrities)\nWikipedia2Vec achieved higher performance on KORE dataset.     Category  Wikipedia2Vec  RDF2Vec ( Ristoski et.al )      IT companies  0.7934  0.743    Hollywood Celebrities  0.6887  0.734    Television Series  0.6415  0.635    Video Games  0.7261  0.669    Chuck Norris  0.6286  0.628    All  0.7084  0.692",
            "title": "Comparison with State of The Art Entity Embeddings Method"
        },
        {
            "location": "/benchmarks/#the-effects-of-parameter-tuning",
            "text": "We also provide benchmark evaluations of Wikipedia2Vec pretrained models,\nwith different training settings to show how the performance varies on various hyper-parameters.\nAll of the pretrained models are available, and you can download them from the pretrained embeddings page .",
            "title": "The Effects of Parameter Tuning"
        },
        {
            "location": "/benchmarks/#link-graph",
            "text": "The link graph model that learns to estimate neighboring entities given an entity\n in the link graph of Wikipedia entities.\nWe compared the performance of the link graph model with the no link graph model to\nsee the effectiveness of the link graphs between entities.\nExcept for the link graph, we set all of the parameters to the same.\nFor both embeddings, we set the  window size  to 5,  iteration  to 10, and negative sampling count  to 15.",
            "title": "Link Graph"
        },
        {
            "location": "/benchmarks/#word-similarity_3",
            "text": "In terms of the performance on Word Similarity task, no link graph model\noutperformed the link graph model.     Dataset  Wikipedia2Vec  Wikipedia2Vec (no link graph)      MEN-TR-3k  0.749  0.7467    RG-65  0.7837  0.7987    SimLex999  0.3815  0.3867    WS-353-ALL  0.6952  0.7009    WS-353-REL  0.6233  0.6304    WS-353-SIM  0.7597  0.7643",
            "title": "Word Similarity"
        },
        {
            "location": "/benchmarks/#word-analogy_3",
            "text": "The link graph model achieves higher performance on both of the Word Analogy task.     Dataset  Wikipedia2Vec  Wikipedia2Vec (no link graph)      GOOGLE ANALOGY (Semantic)  0.7892  0.7804    GOOGLE ANALOGY (Syntactic)  0.6812  0.6703",
            "title": "Word Analogy"
        },
        {
            "location": "/benchmarks/#entity-relatedness_1",
            "text": "Unsurprisingly, without link graph, the model shows significantly huge drop in its\nperformance in Entity Relatedness tasks, because of the lacking of the information\nabout the entity relationship.     Dataset  Wikipedia2Vec  Wikipedia2Vec (no link graph)      KORE  0.6905  0.5892",
            "title": "Entity Relatedness"
        },
        {
            "location": "/benchmarks/#window-size",
            "text": "Previous work shows that the window size for word embeddings training does matter.\nWe conducted evaluation on Wikipedia2Vec models with different window sizes,\nto see how important the window size is for the performance on intrinsic evaluation tasks.  We compare the performance of  enwiki_20180420 (300d)  (Wikipedia2Vec (window=5)) with the one of enwiki_20180420_win10 (300d)  (Wikipedia2Vec (window=10)).  For both embeddings, we set the  iteration  to 10, and negative sampling count  to 15.\nFor training, we only use English Wikipedia dump, without adding any additional large-scale corpora.",
            "title": "Window Size"
        },
        {
            "location": "/benchmarks/#word-similarity_4",
            "text": "Our experimental results show that larger window size seems to improve the\nperformance on Word Similarity, on the all benchmarks except SimLex999.  It should be noted that  Chiu et al.  showed\nthat the performance on word similarity benchmarks like MEN-TR-3k have negative correlations with downstream tasks, while only SimLex999 has high correlation with extrinsic measures.\nThey also observed that decreasing window size improves the performance on downstream tasks, while it leads to deterioration of performance on most of the Word Similarity benchmarks.     Dataset  Wikipedia2Vec (window=5)  Wikipedia2Vec (window=10)      MEN-TR-3k  0.749  0.7541    RG-65  0.7837  0.7861    SimLex999  0.3815  0.3578    WS-353-ALL  0.6952  0.71    WS-353-REL  0.6233  0.6435    WS-353-SIM  0.7597  0.7848",
            "title": "Word Similarity"
        },
        {
            "location": "/benchmarks/#word-analogy_4",
            "text": "Dataset  Wikipedia2Vec (window=5)  Wikipedia2Vec (window=10)      GOOGLE ANALOGY (Semantic)  0.7892  0.789    GOOGLE ANALOGY (Syntactic)  0.6812  0.6529",
            "title": "Word Analogy"
        },
        {
            "location": "/benchmarks/#entity-relatedness_2",
            "text": "Dataset  Wikipedia2Vec (window=5)  Wikipedia2Vec (window=10)      KORE  0.6905  0.6811",
            "title": "Entity Relatedness"
        },
        {
            "location": "/benchmarks/#multilingual-evaluation",
            "text": "We evaluate our word embeddings (300d) on German, French, Spanish, Portuguese, Polish and Chinese\nword analogy task.",
            "title": "Multilingual Evaluation"
        },
        {
            "location": "/benchmarks/#multilingual-word-analogy-dataset",
            "text": "We use the same word analogy dataset as  Grave et.al .   Google Analogy Dataset (German)  Google Analogy Dataset (French)  Google Analogy Dataset (Spanish)  Google Analogy Dataset (Portuguese)  Google Analogy Dataset (Polish)  Google Analogy Dataset (Chinese)",
            "title": "Multilingual Word Analogy Dataset"
        },
        {
            "location": "/benchmarks/#multilingual-word-analogy-results",
            "text": "The results of word analogy task evaluation are as follows.\nFollowing  Grave et.al , we restrict the vocabulary for the analogy tasks to the 200,000 most frequent words from the training data.     language  Wikipedia2Vec  fastText ( Grave et.al )      German  0.617  0.61    French  0.68  0.642    Spanish  0.574  0.574    Portuguese  0.53  0.54    Polish  0.516  0.534    Chinese  0.572  0.631",
            "title": "Multilingual Word Analogy Results"
        }
    ]
}