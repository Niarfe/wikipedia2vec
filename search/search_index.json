{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Wikipedia2Vec Star Introduction \uf0c1 Wikipedia2Vec is a tool used for obtaining embeddings (vector representations) of words and entities from Wikipedia. It is developed and maintained by Studio Ousia . This tool enables you to learn embeddings that map words and entities into a unified continuous vector space. The embeddings can be used as word embeddings, entity embeddings, and the unified embeddings of words and entities. They are used in the state-of-the-art models of various tasks such as entity linking , named entity recognition , entity relatedness , and question answering . The embeddings can be easily trained by a single command with a publicly available Wikipedia dump as input. The code is implemented in Python, and optimized using Cython and BLAS. Pretrained embeddings for 12 languages can be downloaded from this page . Reference \uf0c1 If you use Wikipedia2Vec in a scientific publication, please cite the following paper: Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yoshiyasu Takefuji, Wikipedia2Vec: An Optimized Implementation for Learning Embeddings from Wikipedia . @article{yamada2018wikipedia2vec, title={Wikipedia2Vec: An Optimized Implementation for Learning Embeddings from Wikipedia}, author = {Yamada, Ikuya and Asai, Akari and Shindo, Hiroyuki and Takeda, Hideaki and Takefuji, Yoshiyasu}, journal={arXiv preprint 1812.06280v1}, year={2018} } License \uf0c1 Apache License 2.0","title":"Home"},{"location":"#introduction","text":"Wikipedia2Vec is a tool used for obtaining embeddings (vector representations) of words and entities from Wikipedia. It is developed and maintained by Studio Ousia . This tool enables you to learn embeddings that map words and entities into a unified continuous vector space. The embeddings can be used as word embeddings, entity embeddings, and the unified embeddings of words and entities. They are used in the state-of-the-art models of various tasks such as entity linking , named entity recognition , entity relatedness , and question answering . The embeddings can be easily trained by a single command with a publicly available Wikipedia dump as input. The code is implemented in Python, and optimized using Cython and BLAS. Pretrained embeddings for 12 languages can be downloaded from this page .","title":"Introduction"},{"location":"#reference","text":"If you use Wikipedia2Vec in a scientific publication, please cite the following paper: Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yoshiyasu Takefuji, Wikipedia2Vec: An Optimized Implementation for Learning Embeddings from Wikipedia . @article{yamada2018wikipedia2vec, title={Wikipedia2Vec: An Optimized Implementation for Learning Embeddings from Wikipedia}, author = {Yamada, Ikuya and Asai, Akari and Shindo, Hiroyuki and Takeda, Hideaki and Takefuji, Yoshiyasu}, journal={arXiv preprint 1812.06280v1}, year={2018} }","title":"Reference"},{"location":"#license","text":"Apache License 2.0","title":"License"},{"location":"commands/","text":"Learning Embeddings \uf0c1 First, you need to download a source Wikipedia dump file (e.g., enwiki-latest-pages-articles.xml.bz2) from Wikimedia Downloads . The English dump file can be obtained by running the following command. % wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2 Note that you do not need to decompress the dump file. Then, the embeddings can be trained from a Wikipedia dump using the train command. % wikipedia2vec train DUMP_FILE OUT_FILE Arguments: DUMP_FILE : The Wikipedia dump file OUT_FILE : The output file Options: --dim-size : The number of dimensions of the embeddings (default: 100) --window : The maximum distance between the target item (word or entity) and the context word to be predicted (default: 5) --iteration : The number of iterations for Wikipedia pages (default: 5) --negative : The number of negative samples (default: 5) --lowercase/--no-lowercase : Whether to lowercase words (default: True) --tokenizer : The name of the tokenizer used to tokenize a text into words. Possible choices are regexp , icu , mecab , and jieba --sent-detect : The sentence detector used to split texts into sentences. Currently, only icu is the possible value (default: None) --min-word-count : A word is ignored if the total frequency of the word is less than this value (default: 10) --min-entity-count : An entity is ignored if the total frequency of the entity appearing as the referent of an anchor link is less than this value (default: 5) --min-paragraph-len : A paragraph is ignored if its length is shorter than this value (default: 5) --category/--no-category : Whether to include Wikipedia categories in the dictionary (default:False) --disambi/--no-disambi : Whether to include disambiguation entities in the dictionary (default:False) --link-graph/--no-link-graph : Whether to learn from the Wikipedia link graph (default: True) --entities-per-page : For processing each page, the specified number of randomly chosen entities are used to predict their neighboring entities in the link graph (default: 10) --link-mentions : Whether to convert entity names into links (default: True) --min-link-prob : An entity name is ignored if the probability of the name appearing as a link is less than this value (default: 0.2) --min-prior-prob : An entity is not registered as a referent of an entity name if the probability of the entity name referring to the entity is less than this value (default: 0.01) --max-mention-len : The maximum number of characters in an entity name (default: 20) --init-alpha : The initial learning rate (default: 0.025) --min-alpha : The minimum learning rate (default: 0.0001) --sample : The parameter that controls the downsampling of frequent words (default: 1e-4) --word-neg-power : Negative sampling of words is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0.75) --entity-neg-power : Negative sampling of entities is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0) --pool-size : The number of worker processes (default: the number of CPUs) The train command internally calls the five commands described below (namely, build_dump_db , build_dictionary , build_link_graph , build_mention_db , and train_embedding ). Building Dump Database \uf0c1 The build_dump_db command creates a database that contains Wikipedia pages each of which consists of texts and anchor links in it. % wikipedia2vec build_dump_db DUMP_FILE OUT_FILE Arguments: DUMP_FILE : The Wikipedia dump file OUT_FILE : The output file Options: --pool-size : The number of worker processes (default: the number of CPUs) Building Dictionary \uf0c1 The build_dictionary command builds a dictionary of words and entities. % wikipedia2vec build_dictionary DUMP_DB_FILE OUT_FILE Arguments: DUMP_DB_FILE : The database file generated using the build_dump_db command OUT_FILE : The output file Options: --lowercase/--no-lowercase : Whether to lowercase words (default: True) --tokenizer : The name of the tokenizer used to tokenize a text into words. Possible choices are regexp , icu , mecab , and jieba --min-word-count : A word is ignored if the total frequency of the word is less than this value (default: 10) --min-entity-count : An entity is ignored if the total frequency of the entity appearing as the referent of an anchor link is less than this value (default: 5) --min-paragraph-len : A paragraph is ignored if its length is shorter than this value (default: 5) --category/--no-category : Whether to include Wikipedia categories in the dictionary (default:False) --disambi/--no-disambi : Whether to include disambiguation entities in the dictionary (default:False) --pool-size : The number of worker processes (default: the number of CPUs) Building Link Graph (Optional) \uf0c1 The build_link_graph command generates a sparse matrix representing the link structure between Wikipedia entities. % wikipedia2vec build_link_graph DUMP_DB_FILE DIC_FILE OUT_FILE Arguments: DUMP_DB_FILE : The database file generated using the build_dump_db command DIC_FILE : The dictionary file generated by the build_dictionary command OUT_FILE : The output file Options: --pool-size : The number of worker processes (default: the number of CPUs) Building Mention DB (Optional) \uf0c1 The build_mention_db command builds a database that contains the mappings of entity names (mentions) and their possible referent entities. % wikipedia2vec build_mention_db DUMP_DB_FILE DIC_FILE OUT_FILE Arguments: DUMP_DB_FILE : The database file generated using the build_dump_db command DIC_FILE : The dictionary file generated by the build_dictionary command OUT_FILE : The output file Options: --min-link-prob : An entity name is ignored if the probability of the name appearing as a link is less than this value (default: 0.2) --min-prior-prob : An entity is not registered as a referent of an entity name if the probability of the entity name referring to the entity is less than this value (default: 0.01) --max-mention-len : The maximum number of characters in an entity name (default: 20) --case-sensitive : Whether to detect entity names in a case sensitive manner (default: False) --tokenizer : The name of the tokenizer used to tokenize a text into words. Possible choices are regexp , icu , mecab , and jieba --pool-size : The number of worker processes (default: the number of CPUs) Learning Embeddings \uf0c1 The train_embedding command runs the training of the embeddings. % wikipedia2vec train_embedding DUMP_DB_FILE DIC_FILE OUT_FILE Arguments: DUMP_DB_FILE : The database file generated using the build_dump_db command DIC_FILE : The dictionary file generated by the build_dictionary command OUT_FILE : The output file Options: --link-graph : The link graph file generated using the build_link_graph command --mention-db : The mention DB file generated using the build_mention_db command --dim-size : The number of dimensions of the embeddings (default: 100) --window : The maximum distance between the target item (word or entity) and the context word to be predicted (default: 5) --iteration : The number of iterations for Wikipedia pages (default: 5) --negative : The number of negative samples (default: 5) --tokenizer : The name of the tokenizer used to tokenize a text into words. Possible values are regexp , icu , mecab , and jieba --sent-detect : The sentence detector used to split texts into sentences. Currently, only icu is the possible value (default: None) --entities-per-page : For processing each page, the specified number of randomly chosen entities are used to predict their neighboring entities in the link graph (default: 10) --init-alpha : The initial learning rate (default: 0.025) --min-alpha : The minimum learning rate (default: 0.0001) --sample : The parameter that controls the downsampling of frequent words (default: 1e-4) --word-neg-power : Negative sampling of words is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0.75) --entity-neg-power : Negative sampling of entities is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0) --pool-size : The number of worker processes (default: the number of CPUs) Saving Embeddings in Text Format \uf0c1 save_text outputs a model in a text format. % wikipedia2vec save_text MODEL_FILE OUT_FILE Arguments: MODEL_FILE : The model file generated by the train_embedding command OUT_FILE : The output file Options: --out-format : The output format. Possible values are default , word2vec , and glove . If word2vec and glove are specified, the format adopted by Word2Vec and GloVe are used, respectively.","title":"Learning Embeddings"},{"location":"commands/#learning-embeddings","text":"First, you need to download a source Wikipedia dump file (e.g., enwiki-latest-pages-articles.xml.bz2) from Wikimedia Downloads . The English dump file can be obtained by running the following command. % wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2 Note that you do not need to decompress the dump file. Then, the embeddings can be trained from a Wikipedia dump using the train command. % wikipedia2vec train DUMP_FILE OUT_FILE Arguments: DUMP_FILE : The Wikipedia dump file OUT_FILE : The output file Options: --dim-size : The number of dimensions of the embeddings (default: 100) --window : The maximum distance between the target item (word or entity) and the context word to be predicted (default: 5) --iteration : The number of iterations for Wikipedia pages (default: 5) --negative : The number of negative samples (default: 5) --lowercase/--no-lowercase : Whether to lowercase words (default: True) --tokenizer : The name of the tokenizer used to tokenize a text into words. Possible choices are regexp , icu , mecab , and jieba --sent-detect : The sentence detector used to split texts into sentences. Currently, only icu is the possible value (default: None) --min-word-count : A word is ignored if the total frequency of the word is less than this value (default: 10) --min-entity-count : An entity is ignored if the total frequency of the entity appearing as the referent of an anchor link is less than this value (default: 5) --min-paragraph-len : A paragraph is ignored if its length is shorter than this value (default: 5) --category/--no-category : Whether to include Wikipedia categories in the dictionary (default:False) --disambi/--no-disambi : Whether to include disambiguation entities in the dictionary (default:False) --link-graph/--no-link-graph : Whether to learn from the Wikipedia link graph (default: True) --entities-per-page : For processing each page, the specified number of randomly chosen entities are used to predict their neighboring entities in the link graph (default: 10) --link-mentions : Whether to convert entity names into links (default: True) --min-link-prob : An entity name is ignored if the probability of the name appearing as a link is less than this value (default: 0.2) --min-prior-prob : An entity is not registered as a referent of an entity name if the probability of the entity name referring to the entity is less than this value (default: 0.01) --max-mention-len : The maximum number of characters in an entity name (default: 20) --init-alpha : The initial learning rate (default: 0.025) --min-alpha : The minimum learning rate (default: 0.0001) --sample : The parameter that controls the downsampling of frequent words (default: 1e-4) --word-neg-power : Negative sampling of words is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0.75) --entity-neg-power : Negative sampling of entities is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0) --pool-size : The number of worker processes (default: the number of CPUs) The train command internally calls the five commands described below (namely, build_dump_db , build_dictionary , build_link_graph , build_mention_db , and train_embedding ).","title":"Learning Embeddings"},{"location":"commands/#building-dump-database","text":"The build_dump_db command creates a database that contains Wikipedia pages each of which consists of texts and anchor links in it. % wikipedia2vec build_dump_db DUMP_FILE OUT_FILE Arguments: DUMP_FILE : The Wikipedia dump file OUT_FILE : The output file Options: --pool-size : The number of worker processes (default: the number of CPUs)","title":"Building Dump Database"},{"location":"commands/#building-dictionary","text":"The build_dictionary command builds a dictionary of words and entities. % wikipedia2vec build_dictionary DUMP_DB_FILE OUT_FILE Arguments: DUMP_DB_FILE : The database file generated using the build_dump_db command OUT_FILE : The output file Options: --lowercase/--no-lowercase : Whether to lowercase words (default: True) --tokenizer : The name of the tokenizer used to tokenize a text into words. Possible choices are regexp , icu , mecab , and jieba --min-word-count : A word is ignored if the total frequency of the word is less than this value (default: 10) --min-entity-count : An entity is ignored if the total frequency of the entity appearing as the referent of an anchor link is less than this value (default: 5) --min-paragraph-len : A paragraph is ignored if its length is shorter than this value (default: 5) --category/--no-category : Whether to include Wikipedia categories in the dictionary (default:False) --disambi/--no-disambi : Whether to include disambiguation entities in the dictionary (default:False) --pool-size : The number of worker processes (default: the number of CPUs)","title":"Building Dictionary"},{"location":"commands/#building-link-graph-optional","text":"The build_link_graph command generates a sparse matrix representing the link structure between Wikipedia entities. % wikipedia2vec build_link_graph DUMP_DB_FILE DIC_FILE OUT_FILE Arguments: DUMP_DB_FILE : The database file generated using the build_dump_db command DIC_FILE : The dictionary file generated by the build_dictionary command OUT_FILE : The output file Options: --pool-size : The number of worker processes (default: the number of CPUs)","title":"Building Link Graph (Optional)"},{"location":"commands/#building-mention-db-optional","text":"The build_mention_db command builds a database that contains the mappings of entity names (mentions) and their possible referent entities. % wikipedia2vec build_mention_db DUMP_DB_FILE DIC_FILE OUT_FILE Arguments: DUMP_DB_FILE : The database file generated using the build_dump_db command DIC_FILE : The dictionary file generated by the build_dictionary command OUT_FILE : The output file Options: --min-link-prob : An entity name is ignored if the probability of the name appearing as a link is less than this value (default: 0.2) --min-prior-prob : An entity is not registered as a referent of an entity name if the probability of the entity name referring to the entity is less than this value (default: 0.01) --max-mention-len : The maximum number of characters in an entity name (default: 20) --case-sensitive : Whether to detect entity names in a case sensitive manner (default: False) --tokenizer : The name of the tokenizer used to tokenize a text into words. Possible choices are regexp , icu , mecab , and jieba --pool-size : The number of worker processes (default: the number of CPUs)","title":"Building Mention DB (Optional)"},{"location":"commands/#learning-embeddings_1","text":"The train_embedding command runs the training of the embeddings. % wikipedia2vec train_embedding DUMP_DB_FILE DIC_FILE OUT_FILE Arguments: DUMP_DB_FILE : The database file generated using the build_dump_db command DIC_FILE : The dictionary file generated by the build_dictionary command OUT_FILE : The output file Options: --link-graph : The link graph file generated using the build_link_graph command --mention-db : The mention DB file generated using the build_mention_db command --dim-size : The number of dimensions of the embeddings (default: 100) --window : The maximum distance between the target item (word or entity) and the context word to be predicted (default: 5) --iteration : The number of iterations for Wikipedia pages (default: 5) --negative : The number of negative samples (default: 5) --tokenizer : The name of the tokenizer used to tokenize a text into words. Possible values are regexp , icu , mecab , and jieba --sent-detect : The sentence detector used to split texts into sentences. Currently, only icu is the possible value (default: None) --entities-per-page : For processing each page, the specified number of randomly chosen entities are used to predict their neighboring entities in the link graph (default: 10) --init-alpha : The initial learning rate (default: 0.025) --min-alpha : The minimum learning rate (default: 0.0001) --sample : The parameter that controls the downsampling of frequent words (default: 1e-4) --word-neg-power : Negative sampling of words is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0.75) --entity-neg-power : Negative sampling of entities is performed based on the probability proportional to the frequency raised to the power specified by this option (default: 0) --pool-size : The number of worker processes (default: the number of CPUs)","title":"Learning Embeddings"},{"location":"commands/#saving-embeddings-in-text-format","text":"save_text outputs a model in a text format. % wikipedia2vec save_text MODEL_FILE OUT_FILE Arguments: MODEL_FILE : The model file generated by the train_embedding command OUT_FILE : The output file Options: --out-format : The output format. Possible values are default , word2vec , and glove . If word2vec and glove are specified, the format adopted by Word2Vec and GloVe are used, respectively.","title":"Saving Embeddings in Text Format"},{"location":"features/","text":"Features \uf0c1 Wikipedia2Vec has two unique features that are designed to effectively learn embeddings from Wikipedia. Extended Skip-Gram Model to Learn Embeddings of Words and Entities \uf0c1 Wikipedia2Vec is based on the Word2vec's skip-gram model that learns to predict neighboring words given each word in corpora. We extend the skip-gram model by adding the following two submodels: The link graph model that learns to estimate neighboring entities given an entity in the link graph of Wikipedia entities. The anchor context model that learns to predict neighboring words given an entity by using a link that points to the entity and its neighboring words. By jointly optimizing the skip-gram model and these two submodels, our model simultaneously learns the embedding of words and entities from Wikipedia. For further details, please refer to our paper: Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation . Automatic Generation of Entity Links \uf0c1 Many entity names in Wikipedia do not appear as links because Wikipedia instructs its contributors to link an entity name if it is the first occurrence in the page . This is problematic for our model because the anchor context model depends on entity links to generate contextual words of entities. To address this, we implement a feature that automatically links entity names that do not appear as links. In particular, it takes all words and phrases, and treats them as candidates of entity names if they exist in the Mention DB that contains mapping of an entity name (e.g., \u201cWashington\u201d) to a set of possible referent entities (e.g., Washington, D.C. and George Washington). Then, it converts an entity name to a link pointing to an entity if the entity name is unambiguous (i.e., there is only one referent entity associated to the entity name in the DB) or the entity is referred by an entity link in the same page.","title":"Features"},{"location":"features/#features","text":"Wikipedia2Vec has two unique features that are designed to effectively learn embeddings from Wikipedia.","title":"Features"},{"location":"features/#extended-skip-gram-model-to-learn-embeddings-of-words-and-entities","text":"Wikipedia2Vec is based on the Word2vec's skip-gram model that learns to predict neighboring words given each word in corpora. We extend the skip-gram model by adding the following two submodels: The link graph model that learns to estimate neighboring entities given an entity in the link graph of Wikipedia entities. The anchor context model that learns to predict neighboring words given an entity by using a link that points to the entity and its neighboring words. By jointly optimizing the skip-gram model and these two submodels, our model simultaneously learns the embedding of words and entities from Wikipedia. For further details, please refer to our paper: Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation .","title":"Extended Skip-Gram Model to Learn Embeddings of Words and Entities"},{"location":"features/#automatic-generation-of-entity-links","text":"Many entity names in Wikipedia do not appear as links because Wikipedia instructs its contributors to link an entity name if it is the first occurrence in the page . This is problematic for our model because the anchor context model depends on entity links to generate contextual words of entities. To address this, we implement a feature that automatically links entity names that do not appear as links. In particular, it takes all words and phrases, and treats them as candidates of entity names if they exist in the Mention DB that contains mapping of an entity name (e.g., \u201cWashington\u201d) to a set of possible referent entities (e.g., Washington, D.C. and George Washington). Then, it converts an entity name to a link pointing to an entity if the entity name is unambiguous (i.e., there is only one referent entity associated to the entity name in the DB) or the entity is referred by an entity link in the same page.","title":"Automatic Generation of Entity Links"},{"location":"install/","text":"Installation \uf0c1 Wikipedia2Vec can be installed from PyPI: % pip install wikipedia2vec Alternatively, you can install the development version of this software from the GitHub repository: % git clone https://github.com/studio-ousia/wikipedia2vec.git % cd wikipedia2vec % pip install Cython % ./cythonize.sh % pip install . Wikipedia2Vec requires the 64-bit version of Python, and can be run on Linux, Windows, and Mac OSX. It currently depends on the following Python libraries: click , jieba , joblib , lmdb , marisa-trie , mwparserfromhell , numpy , scipy , six , and tqdm . If you want to train embeddings on your machine, it is highly recommended to install a BLAS library. We recommend using OpenBLAS or Intel Math Kernel Library . Note that, the BLAS library needs to be recognized properly from SciPy. This can be confirmed by using the following command: % python -c 'import scipy; scipy.show_config()' To process Japanese Wikipedia dumps, it is also required to install MeCab and its Python binding . Furthermore, to use ICU library to split either words or sentences or both, you need to install the C/C++ ICU library and the PyICU library.","title":"Installation"},{"location":"install/#installation","text":"Wikipedia2Vec can be installed from PyPI: % pip install wikipedia2vec Alternatively, you can install the development version of this software from the GitHub repository: % git clone https://github.com/studio-ousia/wikipedia2vec.git % cd wikipedia2vec % pip install Cython % ./cythonize.sh % pip install . Wikipedia2Vec requires the 64-bit version of Python, and can be run on Linux, Windows, and Mac OSX. It currently depends on the following Python libraries: click , jieba , joblib , lmdb , marisa-trie , mwparserfromhell , numpy , scipy , six , and tqdm . If you want to train embeddings on your machine, it is highly recommended to install a BLAS library. We recommend using OpenBLAS or Intel Math Kernel Library . Note that, the BLAS library needs to be recognized properly from SciPy. This can be confirmed by using the following command: % python -c 'import scipy; scipy.show_config()' To process Japanese Wikipedia dumps, it is also required to install MeCab and its Python binding . Furthermore, to use ICU library to split either words or sentences or both, you need to install the C/C++ ICU library and the PyICU library.","title":"Installation"},{"location":"pretrained/","text":"Pretrained Embeddings \uf0c1 We provide pretrained embeddings for 12 languages in binary and text format. The binary files can be loaded using the Wikipedia2Vec.load() method (see Basic Usage ). The text files are compatible with the text format of Word2vec . Therefore, these files can be loaded using other libraries such as Gensim's load_word2vec_format() . In the text files, all entities have a prefix ENTITY/ to distinguish them from words. English \uf0c1 enwiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt) 500d (bin) 500d (txt) enwiki_20180420_nolg (window=5, iteration=10, negative=15, no link graph): 100d (bin) 100d (txt) 300d (bin) 300d (txt) 500d (bin) 500d (txt) enwiki_20180420_win10 (window=10, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt) 500d (bin) 500d (txt) Arabic \uf0c1 arwiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt) Chinese \uf0c1 zhwiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt) Dutch \uf0c1 nlwiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt) French \uf0c1 frwiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt) German \uf0c1 dewiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt) Italian \uf0c1 itwiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt) Japanese \uf0c1 jawiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt) Polish \uf0c1 plwiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt) Portuguese \uf0c1 ptwiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt) Russian \uf0c1 ruwiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt) Spanish \uf0c1 eswiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt)","title":"Pretrained Embeddings"},{"location":"pretrained/#pretrained-embeddings","text":"We provide pretrained embeddings for 12 languages in binary and text format. The binary files can be loaded using the Wikipedia2Vec.load() method (see Basic Usage ). The text files are compatible with the text format of Word2vec . Therefore, these files can be loaded using other libraries such as Gensim's load_word2vec_format() . In the text files, all entities have a prefix ENTITY/ to distinguish them from words.","title":"Pretrained Embeddings"},{"location":"pretrained/#english","text":"enwiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt) 500d (bin) 500d (txt) enwiki_20180420_nolg (window=5, iteration=10, negative=15, no link graph): 100d (bin) 100d (txt) 300d (bin) 300d (txt) 500d (bin) 500d (txt) enwiki_20180420_win10 (window=10, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt) 500d (bin) 500d (txt)","title":"English"},{"location":"pretrained/#arabic","text":"arwiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt)","title":"Arabic"},{"location":"pretrained/#chinese","text":"zhwiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt)","title":"Chinese"},{"location":"pretrained/#dutch","text":"nlwiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt)","title":"Dutch"},{"location":"pretrained/#french","text":"frwiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt)","title":"French"},{"location":"pretrained/#german","text":"dewiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt)","title":"German"},{"location":"pretrained/#italian","text":"itwiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt)","title":"Italian"},{"location":"pretrained/#japanese","text":"jawiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt)","title":"Japanese"},{"location":"pretrained/#polish","text":"plwiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt)","title":"Polish"},{"location":"pretrained/#portuguese","text":"ptwiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt)","title":"Portuguese"},{"location":"pretrained/#russian","text":"ruwiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt)","title":"Russian"},{"location":"pretrained/#spanish","text":"eswiki_20180420 (window=5, iteration=10, negative=15): 100d (bin) 100d (txt) 300d (bin) 300d (txt)","title":"Spanish"},{"location":"usage/","text":"Basic Usage \uf0c1 >>> from wikipedia2vec import Wikipedia2Vec >>> wiki2vec = Wikipedia2Vec.load(MODEL_FILE) >>> wiki2vec.get_word_vector('the') memmap([ 0.01617998, -0.03325786, -0.01397999, -0.00150471, 0.03237337, ... -0.04226106, -0.19677088, -0.31087297, 0.1071524 , -0.09824426], dtype=float32) >>> wiki2vec.get_entity_vector('Scarlett Johansson') memmap([-0.19793572, 0.30861306, 0.29620451, -0.01193621, 0.18228433, ... 0.04986198, 0.24383858, -0.01466644, 0.10835337, -0.0697331 ], dtype=float32) >>> wiki2vec.most_similar(wiki2vec.get_word('yoda'), 5) [(<Word yoda>, 1.0), (<Entity Yoda>, 0.84333622), (<Word darth>, 0.73328167), (<Word kenobi>, 0.7328127), (<Word jedi>, 0.7223742)] >>> wiki2vec.most_similar(wiki2vec.get_entity('Scarlett Johansson'), 5) [(<Entity Scarlett Johansson>, 1.0), (<Entity Natalie Portman>, 0.75090045), (<Entity Eva Mendes>, 0.73651594), (<Entity Emma Stone>, 0.72868186), (<Entity Cameron Diaz>, 0.72390842)]","title":"Basic Usage"},{"location":"usage/#basic-usage","text":">>> from wikipedia2vec import Wikipedia2Vec >>> wiki2vec = Wikipedia2Vec.load(MODEL_FILE) >>> wiki2vec.get_word_vector('the') memmap([ 0.01617998, -0.03325786, -0.01397999, -0.00150471, 0.03237337, ... -0.04226106, -0.19677088, -0.31087297, 0.1071524 , -0.09824426], dtype=float32) >>> wiki2vec.get_entity_vector('Scarlett Johansson') memmap([-0.19793572, 0.30861306, 0.29620451, -0.01193621, 0.18228433, ... 0.04986198, 0.24383858, -0.01466644, 0.10835337, -0.0697331 ], dtype=float32) >>> wiki2vec.most_similar(wiki2vec.get_word('yoda'), 5) [(<Word yoda>, 1.0), (<Entity Yoda>, 0.84333622), (<Word darth>, 0.73328167), (<Word kenobi>, 0.7328127), (<Word jedi>, 0.7223742)] >>> wiki2vec.most_similar(wiki2vec.get_entity('Scarlett Johansson'), 5) [(<Entity Scarlett Johansson>, 1.0), (<Entity Natalie Portman>, 0.75090045), (<Entity Eva Mendes>, 0.73651594), (<Entity Emma Stone>, 0.72868186), (<Entity Cameron Diaz>, 0.72390842)]","title":"Basic Usage"}]}